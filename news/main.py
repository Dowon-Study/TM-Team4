from django.http import JsonResponse
from django.views.decorators.http import require_GET
from datetime import datetime
import requests
from dotenv import load_dotenv
import os
from bs4 import BeautifulSoup
import json
from pathlib import Path
from difflib import SequenceMatcher
import csv
from sentence_transformers import SentenceTransformer, util
import hashlib
from .crawler import (convert_to_desktop_url, extract_article_paragraphs, classify_label,)

load_dotenv()  # í™˜ê²½ ë³€ìˆ˜(.env) ë¶ˆëŸ¬ì˜¤ê¸°
model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2") # Sentence Transformer ëª¨ë¸ ë¡œë“œ (í•œ ë²ˆë§Œ ë¡œë“œ)

# ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„± (ì• 5ë¬¸ì¥)
def make_paragraph_key(sentences):
    text = " ".join(sentences[:5])
    return text

# ì¤‘ë³µ ê²€ì‚¬ í•¨ìˆ˜ (ì œëª© + ì• 5ë¬¸ì¥ ê¸°ì¤€ + ì„ë² ë”© ë²¡í„° ë¹„êµ)
def check_duplicate_paragraph(paragraph_sentences, title, collected_paragraphs, threshold=0.85, embed_threshold=0.85):
    summary_part = make_paragraph_key(paragraph_sentences)
    summary_part_with_title = title + "\n" + summary_part
    key_hash = hashlib.md5(summary_part.encode('utf-8')).hexdigest()

    # 1. í•´ì‹œ ì¤‘ë³µ ê²€ì‚¬
    if key_hash in collected_paragraphs:
        return True

    # 2. ë¬¸ìì—´ ìœ ì‚¬ë„ ê²€ì‚¬ (SequenceMatcher ì‚¬ìš©)
    for prev in collected_paragraphs:
        if summary_part_with_title[:30] == prev[:30]:
            return True
        if SequenceMatcher(None, summary_part_with_title, prev).ratio() >= threshold:
            return True

    # 3. ì˜ë¯¸ ìœ ì‚¬ë„ ê²€ì‚¬ (ì„ë² ë”© ë²¡í„° ì‚¬ìš©)
    key_embed = model.encode(summary_part_with_title, convert_to_tensor=True)
    for prev_text in collected_paragraphs:
        prev_embed = model.encode(prev_text, convert_to_tensor=True)
        embed_sim = util.pytorch_cos_sim(key_embed, prev_embed).item()
        if embed_sim >= embed_threshold:
            return True

    collected_paragraphs.append(summary_part_with_title)
    return False

@require_GET  # GET ë©”ì†Œë“œë¥¼ ì´ìš©í•´ API ìš”ì²­
def get_news_by_date(request):
    date = request.GET.get('date')
    query = request.GET.get('query', 'ë‰´ìŠ¤')
    fuzzy = request.GET.get('fuzzy_date', 'false').lower() == 'true'

    if not date:
        return JsonResponse({'error': 'ë‚ ì§œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'}, status=400)

    client_id = os.getenv("NAVER_CLIENT_ID")
    client_secret = os.getenv("NAVER_CLIENT_SECRET")

    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }

    input_date = datetime.strptime(date, "%Y-%m-%d")
    result_data = []
    collected_paragraphs = []
    start = 1
    total_news_needed = 1000
    valid_news_count = 0

    # CSV ì €ì¥ í´ë” ë° íŒŒì¼ ì„¤ì •
    csv_output_dir = Path(__file__).resolve().parent / "outputs_csv" / date
    csv_output_dir.mkdir(parents=True, exist_ok=True)
    csv_output_path = csv_output_dir / f"{date}_{query}.csv"
    write_header = not csv_output_path.exists()

    while valid_news_count < total_news_needed and start <= 1000:
        params = {
            "query": query,
            "display": 100,
            "start": start,
            "sort": "date"
        }

        response = requests.get(url, headers=headers, params=params)
        if response.status_code != 200:
            return JsonResponse({'error': 'API ìš”ì²­ ì‹¤íŒ¨'}, status=500)

        news_data = response.json().get('items', [])
        if not news_data:
            break

        print(f"API ìš”ì²­: start={start} / ë°›ì•„ì˜¨ ê¸°ì‚¬ ìˆ˜: {len(news_data)}")

        for item in news_data:
            pub_date_str = item['pubDate']
            pub_date = datetime.strptime(pub_date_str, "%a, %d %b %Y %H:%M:%S %z")

            # ë‚ ì§œ í•„í„° (fuzzy ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì‘ë™)
            if not fuzzy:
                if pub_date.date() != input_date.date():
                    continue
            else:
                if abs((pub_date.date() - input_date.date()).days) > 2:
                    continue

            link = item['link']
            if "n.news.naver.com" in link:
                link = convert_to_desktop_url(link)

            if "news.naver.com" not in link:
                print("ì™¸ë¶€ ë‰´ìŠ¤ ìŠ¤í‚µ")
                continue

            title = BeautifulSoup(item['title'], "html.parser").get_text()
            paragraph_sentences = extract_article_paragraphs(link)

            print(f"ğŸ”— {link}")
            print(f"ğŸ“„ ë³¸ë¬¸ ìš”ì•½: {paragraph_sentences[:5]}")  # ì• 5ë¬¸ì¥ ìš”ì•½

            if isinstance(paragraph_sentences, str):
                print(f"[í¬ë¡¤ë§ ì‹¤íŒ¨] {link} â†’ ì´ìœ : {paragraph_sentences}")
                continue

            if len(paragraph_sentences) < 5:
                print("ë¬¸ì¥ ìˆ˜ ë¶€ì¡± â†’ ì œì™¸")
                continue

            # ì¤‘ë³µ ê²€ì‚¬
            if check_duplicate_paragraph(paragraph_sentences, title, collected_paragraphs):
                print("ìœ ì‚¬ ë³¸ë¬¸ ìŠ¤í‚µ")
                continue

            combined_text = "\n".join(paragraph_sentences)
            collected_paragraphs.append(title + "\n" + "\n".join(paragraph_sentences[:2]))
            label = classify_label(combined_text + " " + title)

            result_data.append({
                "title": title,
                "text": combined_text,
                "label": label
            })

            # JSON ì €ì¥ ë¡œì§
            sentence_info = []
            for idx, sent in enumerate(paragraph_sentences, start=1):
                sentence_info.append({
                    "sentenceNo": idx,
                    "sentenceContent": sent,
                    "sentenceSize": len(sent)
                })

            news_id = f"{date}_{query}_{valid_news_count + 1:03}"
            safe_news_id = news_id.replace("/", "-").replace("\\", "-")

            output_data = {
                "sourceDataInfo": {
                    "newsID": f"LC_M09_{safe_news_id}",
                    "newsCategory": label,
                    "newsSubcategory": "",
                    "newsTitle": title,
                    "newsSubTitle": "null",
                    "newsContent": combined_text,
                    "partNum": "P1",
                    "useType": 0,
                    "processType": "D",
                    "processPattern": "11",
                    "processLevel": "í•˜",
                    "sentenceCount": len(sentence_info),
                    "sentenceInfo": sentence_info
                },
                "labeledDataInfo": {
                    "newTitle": title,
                    "clickbaitClass": 0,
                    "referSentenceInfo": [
                        {
                            "sentenceNo": s["sentenceNo"],
                            "referSentenceyn": "N"
                        } for s in sentence_info
                    ]
                }
            }

            output_dir = Path(__file__).resolve().parent / "outputs_json" / date
            output_dir.mkdir(parents=True, exist_ok=True)
            output_path = output_dir / f"{safe_news_id}.json"

            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)

            # CSV ì €ì¥ ë¡œì§ (ë¬¸ì¥ ë‹¨ìœ„)
            with open(csv_output_path, "a", newline='', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile)
                if write_header:
                    writer.writerow(["newsID", "title", "label", "sentenceNo", "sentenceContent", "sentenceSize"])
                    write_header = False

                for sentence in sentence_info:
                    writer.writerow([
                        f"LC_M09_{safe_news_id}",
                        title,
                        label,
                        sentence["sentenceNo"],
                        sentence["sentenceContent"],
                        sentence["sentenceSize"]
                    ])

            valid_news_count += 1

            if valid_news_count >= total_news_needed:
                break

        start += 100

    return JsonResponse({"data": result_data}) 